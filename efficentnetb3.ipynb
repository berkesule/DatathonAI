{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Açıklama: Eğitimi colab üzerinden yaptım . Drivedan veriyi çekip colab üzerinden unzip felan yapıyordum ve 5 farklı hesap kullanıyordum o yüzden kod ve adresler çok karışıktı.Kodu düzenleyip kaggle'a aktardım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En iyi modelin eğitiminde \n",
    "\n",
    "epoch5 : f1 0.925  \n",
    "\n",
    "epoch10 : f1 0.9393\n",
    "\n",
    "epoch14 : f1 0.9422 ---> bunu yolladım"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab kodunuda paylaştım oradan çıktılara bakabilirsiniz. Colab kodunda eğitim 2. epoch'tan başladı çünkü  eğitirken 2. epochta yanlışlıkla eğitimi durdurdum. 1 epoch'taki kayıttan devam ettirdim ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:23:56.304026Z",
     "iopub.status.busy": "2024-12-16T06:23:56.303538Z",
     "iopub.status.idle": "2024-12-16T06:24:01.864700Z",
     "shell.execute_reply": "2024-12-16T06:24:01.863742Z",
     "shell.execute_reply.started": "2024-12-16T06:23:56.303961Z"
    },
    "id": "tFxGkj6C3dYB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:24:01.866788Z",
     "iopub.status.busy": "2024-12-16T06:24:01.866287Z",
     "iopub.status.idle": "2024-12-16T06:24:01.894107Z",
     "shell.execute_reply": "2024-12-16T06:24:01.892972Z",
     "shell.execute_reply.started": "2024-12-16T06:24:01.866749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "csv_path = \"/kaggle/input/datathon-ai-qualification-round/train_data.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "label_mapping = {'Ankara': 0, 'Istanbul': 1, 'Izmir': 2}\n",
    "data['city'] = data['city'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:24:01.895717Z",
     "iopub.status.busy": "2024-12-16T06:24:01.895402Z",
     "iopub.status.idle": "2024-12-16T06:24:01.909110Z",
     "shell.execute_reply": "2024-12-16T06:24:01.908370Z",
     "shell.execute_reply.started": "2024-12-16T06:24:01.895689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['city'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:24:01.912101Z",
     "iopub.status.busy": "2024-12-16T06:24:01.911453Z",
     "iopub.status.idle": "2024-12-16T06:24:01.924931Z",
     "shell.execute_reply": "2024-12-16T06:24:01.924151Z",
     "shell.execute_reply.started": "2024-12-16T06:24:01.912074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_csv_path = \"/kaggle/working/train.csv\"\n",
    "test_csv_path = \"/kaggle/working/test.csv\"\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation kodu: horizanlar flip , color jitter ve random crop içerir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:24:01.926080Z",
     "iopub.status.busy": "2024-12-16T06:24:01.925846Z",
     "iopub.status.idle": "2024-12-16T06:25:12.700410Z",
     "shell.execute_reply": "2024-12-16T06:25:12.699454Z",
     "shell.execute_reply.started": "2024-12-16T06:24:01.926057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "images_dir = \"/kaggle/input/datathon-ai-qualification-round/train/train\"\n",
    "train_csv = \"/kaggle/working/train.csv\"\n",
    "test_csv = \"/kaggle/working/test.csv\"\n",
    "augmented_dir = \"/kaggle/working/augmented_train\"\n",
    "augmented_csv = \"/kaggle/working/augmented_train.csv\"\n",
    "\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    filename = row['filename']\n",
    "    city = row['city']\n",
    "    \n",
    "    image_path = os.path.join(images_dir, filename)\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        original_save_path = os.path.join(augmented_dir, filename)\n",
    "        img.save(original_save_path)\n",
    "        augmented_data.append({\"filename\": filename, \"city\": city})\n",
    "        \n",
    "        flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        flipped_filename = f\"flipped_{filename}\"\n",
    "        flipped_save_path = os.path.join(augmented_dir, flipped_filename)\n",
    "        flipped_img.save(flipped_save_path)\n",
    "        augmented_data.append({\"filename\": flipped_filename, \"city\": city})\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found in {images_dir}\")\n",
    "\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "augmented_df.to_csv(augmented_csv, index=False)\n",
    "\n",
    "print(f\"Augmentation işlemi tamamlandı. Veriler {augmented_dir} klasörüne ve {augmented_csv} dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##test kumesinide aynı klasore tasidim\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "test_csv = \"/kaggle/working/test.csv\"\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "def copy_test_images(df, images_dir, augmented_dir):\n",
    "    for index, row in df.iterrows():\n",
    "        filename = row['filename']\n",
    "        city = row['city']\n",
    "\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            img = Image.open(image_path)\n",
    "            save_path = os.path.join(augmented_dir, filename)\n",
    "            img.save(save_path)\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found in {images_dir}\")\n",
    "\n",
    "copy_test_images(test_df, images_dir, augmented_dir)\n",
    "\n",
    "print(f\"Test verilerindeki görüntüler {augmented_dir} klasörüne kopyalandı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance\n",
    "import random\n",
    "\n",
    "images_dir = \"/content/augmented_train\"\n",
    "train_csv = \"/content/drive/MyDrive/Datathonaİ/datasets/dataset/augmented_train.csv\"\n",
    "augmented_dir = \"/content/augmented_train\"\n",
    "augmented_csv = \"/content/drive/MyDrive/Datathonaİ/datasets/dataset/augmented_trainbg.csv\"\n",
    "\n",
    "\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    filename = row['filename']\n",
    "    city = row['city']\n",
    "\n",
    "    image_path = os.path.join(images_dir, filename)\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        img = Image.open(image_path).convert(\"RGB\")  \n",
    "\n",
    "        \"\"\"\n",
    "        original_save_path = os.path.join(augmented_dir, filename)\n",
    "        img.save(original_save_path)\n",
    "        \"\"\"\n",
    "        augmented_data.append({\"filename\": filename, \"city\": city})\n",
    "\n",
    "\n",
    "        brightness = ImageEnhance.Brightness(img).enhance(random.uniform(0.8, 1.2))\n",
    "        contrast = ImageEnhance.Contrast(brightness).enhance(random.uniform(0.8, 1.2))\n",
    "        saturation = ImageEnhance.Color(contrast).enhance(random.uniform(0.8, 1.2))\n",
    "        jittered_img = ImageEnhance.Brightness(saturation).enhance(random.uniform(0.8, 1.2))\n",
    "\n",
    "        jittered_filename = f\"jittered_{filename}\"\n",
    "        jittered_save_path = os.path.join(augmented_dir, jittered_filename)\n",
    "        jittered_img.save(jittered_save_path)\n",
    "        augmented_data.append({\"filename\": jittered_filename, \"city\": city})\n",
    "\n",
    "\n",
    "        width, height = img.size\n",
    "        crop_size = 540\n",
    "        if width >= crop_size and height >= crop_size:\n",
    "            left = random.randint(0, width - crop_size)\n",
    "            top = random.randint(0, height - crop_size)\n",
    "            cropped_img = img.crop((left, top, left + crop_size, top + crop_size))\n",
    "            resized_img = cropped_img.resize((640, 640))\n",
    "\n",
    "            cropped_filename = f\"cropped_{filename}\"\n",
    "            cropped_save_path = os.path.join(augmented_dir, cropped_filename)\n",
    "            resized_img.save(cropped_save_path)\n",
    "            augmented_data.append({\"filename\": cropped_filename, \"city\": city})\n",
    "        else:\n",
    "            print(f\"Warning: {filename} boyutları kırpma için uygun değil.\")\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found in {images_dir}\")\n",
    "\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "augmented_df.to_csv(augmented_csv, index=False)\n",
    "\n",
    "print(f\"Augmentation işlemi tamamlandı. Veriler {augmented_dir} klasörüne ve {augmented_csv} dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:25:27.143008Z",
     "iopub.status.busy": "2024-12-16T06:25:27.142760Z",
     "iopub.status.idle": "2024-12-16T06:25:27.148970Z",
     "shell.execute_reply": "2024-12-16T06:25:27.148061Z",
     "shell.execute_reply.started": "2024-12-16T06:25:27.142984Z"
    },
    "id": "zVkBrDuy3dYE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['filename']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['city']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "img_dir = \"/kaggle/working/augmented_train\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:25:27.150322Z",
     "iopub.status.busy": "2024-12-16T06:25:27.150004Z",
     "iopub.status.idle": "2024-12-16T06:25:27.246325Z",
     "shell.execute_reply": "2024-12-16T06:25:27.245432Z",
     "shell.execute_reply.started": "2024-12-16T06:25:27.150263Z"
    },
    "id": "TK1JdU6E3dYG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "label_mapping = {'Ankara': 0, 'Istanbul': 1, 'Izmir': 2}\n",
    "##train test spliti her seferinde çalıştırmadım hesap değiştirdiğim zamanlar çalıştırdım\n",
    "#train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['city'], random_state=42)\n",
    "\n",
    "train_csv_path  = \"/content/drive/MyDrive/Datathonaİ/datasets/dataset/augmented_trainbg.csv\"\n",
    "test_csv_path  = \"/kaggle/working/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "# yorum satırına alınan yerleri hesap değiştireceğim zamanlar yani yeni runtime açtığımada çalıştırıyordum\n",
    "#train_df['city'] = train_df['city'].map(label_mapping)\n",
    "\n",
    "##train_df.to_csv(train_csv_path, index=False)\n",
    "#test_df.to_csv(test_csv_path, index=False)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aşağıdaki kod en iyi sonuç aldığım model'in eğitim kodudur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:25:27.252142Z",
     "iopub.status.busy": "2024-12-16T06:25:27.251671Z",
     "iopub.status.idle": "2024-12-16T06:25:31.036545Z",
     "shell.execute_reply": "2024-12-16T06:25:31.035032Z",
     "shell.execute_reply.started": "2024-12-16T06:25:27.252102Z"
    },
    "id": "qqhIznsR3dYL",
    "outputId": "6dd8ac6f-8c55-4c50-dc8a-b870223461f1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CityDataset(train_df, img_dir, transform=transform)\n",
    "test_dataset = CityDataset(test_df, img_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "model = models.efficientnet_b3(pretrained=True)  \n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "# Classifier'ı değiştirme\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3),  # Overfitting'i önlemek için Dropout\n",
    "    nn.Linear(model.classifier[1].in_features, num_classes)\n",
    ")\n",
    "  # Son lineer katmanı değiştiriyoruz\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#GPU memoryinin yetmesi için bu 4 katmanı dondurdum colab t4 gpu tam yetti\n",
    "for name, param in model.named_parameters():\n",
    "    if \"features.0\" in name or \"features.1\" in name or \"features.2\" in name or \"features.3\" in name :\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:25:31.038788Z",
     "iopub.status.busy": "2024-12-16T06:25:31.038430Z",
     "iopub.status.idle": "2024-12-16T06:25:31.045200Z",
     "shell.execute_reply": "2024-12-16T06:25:31.044281Z",
     "shell.execute_reply.started": "2024-12-16T06:25:31.038749Z"
    },
    "id": "-xAxDNZJ3dYN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scheduler, path=\"/kaggle/working/efficentnetb3.pth\"):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() \n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path=\"/content/efficentnet2.pth\"):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict']) #cpu da  calisacagi zaman map locarion = cpu eklemek gerekiyor\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict']) \n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint yükleniyor: {path}, Epoch: {start_epoch}\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(\"Checkpoint bulunamadı. Eğitim sıfırdan başlıyor.\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kodda f1 scoru en yüksek olan modeli kaydediyor ve lr scheduler f1 scoruna göre lr yi ayarlıyor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:25:31.047558Z",
     "iopub.status.busy": "2024-12-16T06:25:31.047005Z",
     "iopub.status.idle": "2024-12-16T06:25:31.398156Z",
     "shell.execute_reply": "2024-12-16T06:25:31.397157Z",
     "shell.execute_reply.started": "2024-12-16T06:25:31.047516Z"
    },
    "id": "Db1DJbRMz_0-",
    "outputId": "f45f9dee-13ec-4a1d-dbf1-48834f5ae475",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=50, checkpoint_interval=3, start_epoch=0):\n",
    "    best_macro_f1 = 0.0\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        model.train()\n",
    "        print(\"--------------------------------------\")\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        for images, labels in train_loader:\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(i)\n",
    "            i += 1\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # test kümesinde dogrulama\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "        for city, idx in label_mapping.items():\n",
    "            print(f\"Class: {city}, Precision: {precision[idx]:.4f}, Recall: {recall[idx]:.4f}, F1: {f1[idx]:.4f}\")\n",
    "\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "        print(f\"Macro F1 Score: {f1_macro:.4f}\")\n",
    "\n",
    "        \n",
    "        scheduler.step(f1_macro)\n",
    "\n",
    "        if f1_macro > best_macro_f1:\n",
    "            best_macro_f1 = f1_macro\n",
    "            print(f\"Yeni en iyi Macro F1 bulundu: {best_macro_f1:.4f}, modeli kaydediyorum...\")\n",
    "            save_checkpoint(epoch + 1, model, optimizer,scheduler)\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "\n",
    "start_epoch = load_checkpoint(model, optimizer, scheduler,  path=\"/content/drive/MyDrive/Datathonaİ/datasets/dataset/efficentnet5.pth\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En iyi modelin en iyi halini alması 14 epoch sürdü**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:25:31.399705Z",
     "iopub.status.busy": "2024-12-16T06:25:31.399355Z"
    },
    "id": "8d-pgn94E3P2",
    "outputId": "4a51cbc9-a811-4b37-e050-633262cd636a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, scheduler, start_epoch=start_epoch, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeli tekrar değerlendiriyoruz kontrol amaçlı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-16T06:14:04.380929Z",
     "iopub.status.idle": "2024-12-16T06:14:04.381218Z",
     "shell.execute_reply": "2024-12-16T06:14:04.381090Z",
     "shell.execute_reply.started": "2024-12-16T06:14:04.381076Z"
    },
    "id": "2qRMpTn1LFrr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "for city, idx in label_mapping.items():\n",
    "    print(f\"Class: {city}, Precision: {precision[idx]:.4f}, Recall: {recall[idx]:.4f}, F1: {f1[idx]:.4f}\")\n",
    "\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Macro F1 Score: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSV dosyasına model tahmini yazdırma kodu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bu kodu modeli bilgisayara indirip cpu üzerinden çalıştırdım .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-16T06:14:04.383917Z",
     "iopub.status.idle": "2024-12-16T06:14:04.384324Z",
     "shell.execute_reply": "2024-12-16T06:14:04.384131Z",
     "shell.execute_reply.started": "2024-12-16T06:14:04.384110Z"
    },
    "id": "u5-aUsFALFrs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['filename']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_name  .\n",
    "\n",
    "test_csv_path = \"test.csv\"\n",
    "img_dir = \"C:/Users/Berke/Desktop/dataset/test/test/\"\n",
    "\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = CityDataset(test_df, img_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test2_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, filenames in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "      \n",
    "        predicted_cities = [city for idx in preds.cpu().numpy() for city, label_idx in label_mapping.items() if label_idx == idx]\n",
    "        test2_preds.extend(zip(filenames, predicted_cities))\n",
    "\n",
    "predictions_df = pd.DataFrame(test2_preds, columns=['filename', 'city'])\n",
    "predictions_df.to_csv(\"C:/Users/Berke/Desktop/dataset/efficentnet3.csv\", index=False)\n",
    "\n",
    "print(\"Tahminler başarıyla CSV dosyasına kaydedildi!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiHxT3Wh3x_K",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10477255,
     "sourceId": 90279,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
